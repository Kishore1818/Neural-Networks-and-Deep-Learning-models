{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0JTFDEojBou"
   },
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#c37235\">Bag of words and Tensorflow tokenizer\n",
    "<font color=darkblue>\n",
    "    \n",
    "    \n",
    "- This is simple approach of bag of Words and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E7do5pgZjPM6"
   },
   "outputs": [],
   "source": [
    "# Tenorflow \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Tenorflow Padding Sequences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['there is a snake in my boot', 'there is boot snake in my house', 'a a a a']\n",
    "labels = [75, 12, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list(['a', 'a', 'a', 'a']),\n",
       "        list(['there', 'is', 'a', 'snake', 'in', 'my', 'boot']),\n",
       "        list(['there', 'is', 'boot', 'snake', 'in', 'my', 'house'])],\n",
       "       dtype=object),\n",
       " [['there', 'is', 'a', 'snake', 'in', 'my', 'boot'],\n",
       "  ['there', 'is', 'boot', 'snake', 'in', 'my', 'house'],\n",
       "  ['a', 'a', 'a', 'a']])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = [tweet.split(' ') for tweet in tweets]\n",
    "unique_words = np.unique(tweets)\n",
    "unique_words, tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'there': 0,\n",
       " 'is': 1,\n",
       " 'a': 2,\n",
       " 'snake': 3,\n",
       " 'in': 4,\n",
       " 'my': 5,\n",
       " 'boot': 6,\n",
       " 'house': 7}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = {}\n",
    "\n",
    "counter = 0\n",
    "for tweet in tweets:\n",
    "  for word in tweet:\n",
    "    if word not in tokenizer:\n",
    "      tokenizer[word] = counter\n",
    "      counter += 1\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 0, 1, 1, 1, 1, 1], [0, 0, 4, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_words = []\n",
    "# count_count = [0]*len(unique_words)\n",
    "\n",
    "for tweet in tweets:\n",
    "  word_count = [0]*(counter)\n",
    "  \n",
    "  #Counts instence of every unique word that appears\n",
    "  for word in tweet:\n",
    "    locWord = tokenizer[word] # Get the index location of the words\n",
    "    word_count[locWord] += 1 # Counts the number of times that word appears\n",
    "    \n",
    "  # Append after finnished counting\n",
    "  bag_words.append(word_count)\n",
    "\n",
    "bag_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 4, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mUuAl3edkn4N"
   },
   "outputs": [],
   "source": [
    "# Twitter length\n",
    "token_len = 50\n",
    "\n",
    "# Create Decorator\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer() # Sets up the Tikenizer which we will feed\n",
    "\n",
    "# Fitting the Tokenizer and building our Corpus\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "# Create our sequence\n",
    "X = tokenizer.texts_to_sequences(tweets)\n",
    "\n",
    "# Padding the text\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=token_len, padding= 'post', truncating='post')\n",
    "\n",
    "# Convert array to Tensor\n",
    "X = tf.constant(X, dtype=tf.int64)\n",
    "y = tf.constant(labels, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "EaG_Mib2ov60",
    "outputId": "1b861bf3-0155-485a-b52a-1bf53f77377b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 50])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JLtnjn6RvBJC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bag-O-Words.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
