{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">COVID Tweets: Deep Neural Network Models\n",
    "\n",
    "- for cleaning COVID_19 tweets: Covid_tweets_clean_wordcloud.ipynb\n",
    "    \n",
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.0em;color:#35c33a\">Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.0em;color:#35c33a\">Load cleaned tweets (Covid_tweets_clean_wordcloud.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>London</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>menyrbie chrisitv</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>advice talk neighbour family exchange phone nu...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>coronavirus australia woolworth give elderly d...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>food stock one empty please dont panic enough ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>ready go supermarket outbreak im paranoid food...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Location     TweetAt                                      OriginalTweet  \\\n",
       "0     London  2020-03-16                                  menyrbie chrisitv   \n",
       "1         UK  2020-03-16  advice talk neighbour family exchange phone nu...   \n",
       "2  Vagabonds  2020-03-16  coronavirus australia woolworth give elderly d...   \n",
       "3        NaN  2020-03-16  food stock one empty please dont panic enough ...   \n",
       "4        NaN  2020-03-16  ready go supermarket outbreak im paranoid food...   \n",
       "\n",
       "            Sentiment  \n",
       "0             Neutral  \n",
       "1            Positive  \n",
       "2            Positive  \n",
       "3            Positive  \n",
       "4  Extremely Negative  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NYC</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>couldnt find hand sanitizer fred meyer turned ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>find protect loved one coronavirus</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>panic buying hit newyork city anxious shopper ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>toiletpaper dunnypaper coronavirus coronavirus...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Location     TweetAt  \\\n",
       "0                  NYC  2020-03-02   \n",
       "1          Seattle, WA  2020-03-02   \n",
       "2                  NaN  2020-03-02   \n",
       "3          Chicagoland  2020-03-02   \n",
       "4  Melbourne, Victoria  2020-03-03   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  trending new yorkers encounter empty supermark...  Extremely Negative  \n",
       "1  couldnt find hand sanitizer fred meyer turned ...            Positive  \n",
       "2                 find protect loved one coronavirus  Extremely Positive  \n",
       "3  panic buying hit newyork city anxious shopper ...            Negative  \n",
       "4  toiletpaper dunnypaper coronavirus coronavirus...             Neutral  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_cleaned = pd.read_csv('/Users/preethamvignesh/Desktop/Work/ML_EIT/Data/corona_nlpdata/covidtweets_train_cleaned.csv')\n",
    "test_cleaned = pd.read_csv('/Users/preethamvignesh/Desktop/Work/ML_EIT/Data/corona_nlpdata/covidtweets_test_cleaned.csv')\n",
    "display(train_cleaned.head(), test_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode the target:\n",
    "y_train = pd.get_dummies(train_cleaned.Sentiment).values\n",
    "y_test = pd.get_dummies(test_cleaned.Sentiment).values\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Modeling Deep Neural Network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Count Vectorizer Models:\n",
    "    \n",
    "- Prepare the data with CountVectorizer method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_cleaned.OriginalTweet.values)\n",
    "\n",
    "X_train = vectorizer.transform(train_cleaned.OriginalTweet.values)\n",
    "X_test = vectorizer.transform(test_cleaned.OriginalTweet.values)\n",
    "\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41134, 52500), (3798, 52500), (41134, 5), (3798, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.0em;color:#c3b235\">Simple One Layer Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "opti = Adam(lr = 0.01)\n",
    "\n",
    "model_simple_count = Sequential()\n",
    "model_simple_count.add(Dense(16, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model_simple_count.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                840016    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 840,101\n",
      "Trainable params: 840,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_simple_count.compile(loss = 'categorical_crossentropy', optimizer = opti, metrics = ['accuracy'])\n",
    "model_simple_count.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2571/2571 [==============================] - 31s 12ms/step - loss: 1.1772 - accuracy: 0.5085 - val_loss: 1.0181 - val_accuracy: 0.6051\n",
      "Epoch 2/2\n",
      "2571/2571 [==============================] - 33s 13ms/step - loss: 0.5688 - accuracy: 0.7962 - val_loss: 1.1155 - val_accuracy: 0.6222\n"
     ]
    }
   ],
   "source": [
    "history_simple_count = model_simple_count.fit(X_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models and history\n",
    "# model_simple_count.save('/Users/preethamvignesh/Downloads/Simple_model_Count.h5')\n",
    "# np.save('/Users/preethamvignesh/Downloads/history_simple_count.npy',history_simple_count.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Mutli layers model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti = Adam(lr = 0.01)\n",
    "\n",
    "model_multi_count = Sequential()\n",
    "model_multi_count.add(Dense(64, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model_multi_count.add(Dense(32, activation = 'relu'))\n",
    "model_multi_count.add(Dense(16, activation = 'relu'))\n",
    "model_multi_count.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                3360064   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 3,362,757\n",
      "Trainable params: 3,362,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_multi_count.compile(loss = 'categorical_crossentropy', optimizer = opti, metrics = ['accuracy'])\n",
    "model_multi_count.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2571/2571 [==============================] - 81s 31ms/step - loss: 1.2084 - accuracy: 0.4929 - val_loss: 0.9789 - val_accuracy: 0.6209\n",
      "Epoch 2/2\n",
      "2571/2571 [==============================] - 57s 22ms/step - loss: 0.6566 - accuracy: 0.7590 - val_loss: 1.0104 - val_accuracy: 0.6230\n"
     ]
    }
   ],
   "source": [
    "history_multi_count = model_multi_count.fit(X_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save models and history\n",
    "# model_multi_count.save('/Users/preethamvignesh/Downloads/model_multi_count.h5')\n",
    "# np.save('/Users/preethamvignesh/Downloads//history_multi_count.npy',history_multi_count.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cleaned['num_words'] = train_cleaned.OriginalTweet.apply(lambda x : len(x.split()))\n",
    "max(train_cleaned['num_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Flatten, GlobalMaxPool1D, Conv1D\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_cleaned.OriginalTweet.values)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_cleaned.OriginalTweet.values)\n",
    "X_test = tokenizer.texts_to_sequences(test_cleaned.OriginalTweet.values)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Single layer Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 30\n",
    "opti = Adam(lr = 0.01)\n",
    "\n",
    "model_simple_embed = Sequential()\n",
    "model_simple_embed.add(Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model_simple_embed.add(Flatten())\n",
    "model_simple_embed.add(Dense(16, activation = 'relu'))\n",
    "model_simple_embed.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 30)            1575780   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 900)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                14416     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 1,590,281\n",
      "Trainable params: 1,590,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_simple_embed.compile(loss = 'categorical_crossentropy', optimizer = opti, metrics = ['accuracy'])\n",
    "model_simple_embed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2571/2571 [==============================] - 43s 17ms/step - loss: 1.2879 - accuracy: 0.4401 - val_loss: 1.0924 - val_accuracy: 0.5432\n",
      "Epoch 2/2\n",
      "2571/2571 [==============================] - 47s 18ms/step - loss: 0.8015 - accuracy: 0.6971 - val_loss: 1.0725 - val_accuracy: 0.6006\n"
     ]
    }
   ],
   "source": [
    "history_simple_embed = model_simple_embed.fit(X_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models and history\n",
    "# model_simple_embed.save('/Users/preethamvignesh/Downloads/model_simple_embed.h5')\n",
    "# np.save('/Users/preethamvignesh/Downloads/history_simple_embed.npy',history_simple_embed.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Multi layer Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 30\n",
    "opti = Adam(lr = 0.01)\n",
    "\n",
    "multi_model_Embed = Sequential()\n",
    "multi_model_Embed.add(Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "multi_model_Embed.add(Flatten())\n",
    "multi_model_Embed.add(Dense(64, activation = 'relu'))\n",
    "multi_model_Embed.add(Dense(32, activation = 'relu'))\n",
    "multi_model_Embed.add(Dense(16, activation = 'relu'))\n",
    "multi_model_Embed.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 30)            1575780   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 900)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                57664     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 1,636,137\n",
      "Trainable params: 1,636,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "multi_model_Embed.compile(loss = 'categorical_crossentropy', optimizer = opti, metrics = ['accuracy'])\n",
    "multi_model_Embed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2571/2571 [==============================] - 48s 18ms/step - loss: 1.3948 - accuracy: 0.3795 - val_loss: 1.0857 - val_accuracy: 0.5558\n",
      "Epoch 2/3\n",
      "2571/2571 [==============================] - 48s 19ms/step - loss: 0.8504 - accuracy: 0.6775 - val_loss: 1.0154 - val_accuracy: 0.6232\n",
      "Epoch 3/3\n",
      "2571/2571 [==============================] - 46s 18ms/step - loss: 0.6491 - accuracy: 0.7703 - val_loss: 1.0692 - val_accuracy: 0.5948\n"
     ]
    }
   ],
   "source": [
    "history_multi_Embed = multi_model_Embed.fit(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models and history\n",
    "# multi_model_Embed.save('/Users/preethamvignesh/Downloads/multi_model_Embed.h5')\n",
    "# np.save('/Users/preethamvignesh/Downloads/history_multi_Embed.npy',history_multi_Embed.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Glove Dictionary Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('/Users/preethamvignesh/Desktop/Work/ML_EIT/Data/corona_nlpdata/glove.6B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 25072 words (27453 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Simple Glove Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "opti = Adam(lr=0.01)\n",
    "\n",
    "model_simple_glove = Sequential()\n",
    "model_simple_glove.add(Embedding(vocab_size, embedding_dim,input_length=maxlen, weights = [embedding_matrix], trainable = False))\n",
    "model_simple_glove.add(Flatten())\n",
    "model_simple_glove.add(Dense(16, activation = 'relu'))\n",
    "model_simple_glove.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 30, 50)            2626300   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                24016     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 2,650,401\n",
      "Trainable params: 24,101\n",
      "Non-trainable params: 2,626,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_simple_glove.compile(loss = 'categorical_crossentropy', optimizer = opti, metrics = ['accuracy'])\n",
    "model_simple_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2571/2571 [==============================] - 4s 2ms/step - loss: 1.4963 - accuracy: 0.3308 - val_loss: 1.4455 - val_accuracy: 0.3273\n",
      "Epoch 2/2\n",
      "2571/2571 [==============================] - 4s 1ms/step - loss: 1.3758 - accuracy: 0.3849 - val_loss: 1.4234 - val_accuracy: 0.3612\n"
     ]
    }
   ],
   "source": [
    "history_simple_glove = model_simple_glove.fit(X_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models and history\n",
    "# model_simple_glove.save('/Users/preethamvignesh/Downloads/model_simple_glove.h5')\n",
    "# np.save('/Users/preethamvignesh/Downloads/history_simple_glove.npy',history_simple_glove.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Mutli layers Glove model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "opti = Adam(lr = 0.01)\n",
    "\n",
    "model_multi_glove = Sequential()\n",
    "model_multi_glove.add(Embedding(vocab_size, embedding_dim,input_length=maxlen, weights = [embedding_matrix], trainable = False))\n",
    "model_multi_glove.add(Flatten())\n",
    "model_multi_glove.add(Dense(64, activation = 'relu'))\n",
    "model_multi_glove.add(Dense(32, activation = 'relu'))\n",
    "model_multi_glove.add(Dense(16, activation = 'relu'))\n",
    "model_multi_glove.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 30, 50)            2626300   \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                96064     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 2,725,057\n",
      "Trainable params: 98,757\n",
      "Non-trainable params: 2,626,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_multi_glove.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model_multi_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2571/2571 [==============================] - 5s 2ms/step - loss: 1.4485 - accuracy: 0.3475 - val_loss: 1.3642 - val_accuracy: 0.3960\n",
      "Epoch 2/2\n",
      "2571/2571 [==============================] - 5s 2ms/step - loss: 1.2744 - accuracy: 0.4498 - val_loss: 1.3486 - val_accuracy: 0.4049\n"
     ]
    }
   ],
   "source": [
    "history_Multi_glove = model_multi_glove.fit(X_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models and history\n",
    "# model_multi_glove.save('/Users/preethamvignesh/Downloads/model_multi_glove.h5')\n",
    "# np.save('/Users/preethamvignesh/Downloads/history_Multi_glove.npy',history_Multi_glove.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Convolutional Neural Networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "embedding_dim = 30\n",
    "opti = Adam(lr=0.01)\n",
    "\n",
    "model_Conv = Sequential()\n",
    "model_Conv.add(Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model_Conv.add(Dropout(0.2))\n",
    "model_Conv.add(Conv1D(16, 3, activation='relu'))\n",
    "model_Conv.add(GlobalMaxPool1D())\n",
    "model_Conv.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 30, 30)            1575780   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 28, 16)            1456      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 1,577,321\n",
      "Trainable params: 1,577,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_Conv.compile(loss = 'categorical_crossentropy', optimizer = opti, metrics = ['accuracy'])\n",
    "model_Conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2571/2571 [==============================] - 49s 19ms/step - loss: 1.2442 - accuracy: 0.4654 - val_loss: 0.9943 - val_accuracy: 0.6174\n",
      "Epoch 2/2\n",
      "2571/2571 [==============================] - 47s 18ms/step - loss: 0.8956 - accuracy: 0.6523 - val_loss: 0.9879 - val_accuracy: 0.6282\n"
     ]
    }
   ],
   "source": [
    "history_Conv = model_Conv.fit(X_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_Conv.save('/Users/preethamvignesh/Downloads/NN_Models/model_Conv.h5')\n",
    "# np.save('/Users/preethamvignesh/Downloads/history_Conv.npy',history_Conv.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#c3b235\">Convolutional with Glove dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "opti = Adam(lr=0.01)\n",
    "\n",
    "model_Conv_glove = Sequential()\n",
    "model_Conv_glove.add(Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim,\n",
    "                           weights = [embedding_matrix],\n",
    "                           input_length=maxlen))\n",
    "model_Conv_glove.add(Dropout(0.2))\n",
    "model_Conv_glove.add(Conv1D(16, 3, activation='relu'))\n",
    "model_Conv_glove.add(GlobalMaxPool1D())\n",
    "model_Conv_glove.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 30, 50)            2626300   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 28, 16)            2416      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 2,628,801\n",
      "Trainable params: 2,628,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_Conv_glove.compile(loss = 'categorical_crossentropy', optimizer = opti, metrics = ['accuracy'])\n",
    "model_Conv_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2571/2571 [==============================] - 75s 29ms/step - loss: 1.2371 - accuracy: 0.4781 - val_loss: 1.0346 - val_accuracy: 0.6045\n",
      "Epoch 2/2\n",
      "2571/2571 [==============================] - 78s 30ms/step - loss: 0.8970 - accuracy: 0.6553 - val_loss: 0.9730 - val_accuracy: 0.6345\n"
     ]
    }
   ],
   "source": [
    "history_Conv_glove = model_Conv_glove.fit(X_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_Conv_glove.save('/Users/preethamvignesh/Downloads/model_Conv_glove.h5')\n",
    "# np.save('/Users/spavot/Documents/Perso/Text classification & Visualization/Models/History/history_Conv_glove.npy',history_Conv_glove.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# import tensorflow as tf\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.layers import LSTM\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.layers import Embedding, Flatten, GlobalMaxPool1D, Conv1D\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from wordcloud import WordCloud\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
